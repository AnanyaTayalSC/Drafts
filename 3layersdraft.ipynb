{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnanyaTayalSC/Drafts/blob/main/3layersdraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "Xph-Bj07ULd9",
        "outputId": "a8df2164-1a75-47d1-ccdf-dd32b4a38e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data shape: (1395, 501, 3)\n",
            "Epoch 1/50, Loss: 174.1758\n",
            "Epoch 2/50, Loss: 172.2973\n",
            "Epoch 3/50, Loss: 172.9182\n",
            "Epoch 4/50, Loss: 173.4011\n",
            "Epoch 5/50, Loss: 170.6591\n",
            "Epoch 6/50, Loss: 167.8648\n",
            "Epoch 7/50, Loss: 167.1109\n",
            "Epoch 8/50, Loss: 167.0874\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1088138124.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# =====================================\n",
        "# 1. Load CSV and build tensor\n",
        "# =====================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/Thesis/Imp.csv'\n",
        "df = pd.read_csv(csv_file_path)  # columns: sample_id, time, var1, var2, var3\n",
        "\n",
        "# Ensure sorting (important!)\n",
        "df = df.sort_values(by=[\"sample_id\", \"time\"])\n",
        "\n",
        "# Pivot to wide format: each (sample_id, time) → row\n",
        "#X = df.pivot(index=[\"sample_id\", \"time\"], columns=None, values=[\"var1\",\"var2\",\"var3\"]).reset_index()\n",
        "\n",
        "# Group by sample_id and collect sequences\n",
        "X_list = []\n",
        "for sid, group in df.groupby(\"sample_id\"):\n",
        "    values = group[[\"var1\",\"var2\",\"var3\"]].values  # shape (T, 3)\n",
        "    X_list.append(values)\n",
        "\n",
        "X = np.stack(X_list)  # final shape (N, T, 3)\n",
        "print(\"Data shape:\", X.shape)  # e.g. (1394, 100, 3)\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 2. Define LSTM Autoencoder\n",
        "# =====================================\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=128, latent_dim=64, num_layers=1):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        _, (h, _) = self.encoder_lstm(x)  # h: (num_layers, batch, hidden_dim)\n",
        "        h = h[-1]                         # (batch, hidden_dim)\n",
        "        z = self.fc(h)                    # (batch, latent_dim)\n",
        "\n",
        "        # Decoder (repeat latent vector across all timesteps)\n",
        "        z_repeated = z.unsqueeze(1).repeat(1, x.size(1), 1)  # (batch, T, latent_dim)\n",
        "        decoded, _ = self.decoder_lstm(z_repeated)\n",
        "        out = self.output_layer(decoded)   # (batch, T, 3)\n",
        "\n",
        "        return out, z\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 3. Train Autoencoder\n",
        "# =====================================\n",
        "model = LSTMAutoencoder(input_dim=3, hidden_dim=128, latent_dim=64)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Put into batches (for large datasets)\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(X_tensor)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        x_batch = batch[0]\n",
        "        optimizer.zero_grad()\n",
        "        reconstructed, _ = model(x_batch)\n",
        "        loss = criterion(reconstructed, x_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 4. Extract embeddings\n",
        "# =====================================\n",
        "with torch.no_grad():\n",
        "    _, embeddings = model(X_tensor)  # (N, latent_dim)\n",
        "    embeddings = embeddings.numpy()\n",
        "\n",
        "print(\"Embeddings shape:\", embeddings.shape)  # (1394, latent_dim)\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 5. Clustering\n",
        "# =====================================\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "print(\"Cluster assignments:\", labels[:20])  # first 20 samples\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# =====================================\n",
        "# 1. Load CSV and build tensor\n",
        "# =====================================\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/Thesis/Imp.csv'\n",
        "df = pd.read_csv(csv_file_path)  # columns: sample_id, time, var1, var2, var3\n",
        "\n",
        "# Ensure sorting (important!)\n",
        "df = df.sort_values(by=[\"sample_id\", \"time\"])\n",
        "\n",
        "# Group by sample_id and collect sequences\n",
        "X_list = []\n",
        "for sid, group in df.groupby(\"sample_id\"):\n",
        "    values = group[[\"var1\",\"var2\",\"var3\"]].values  # shape (T, 3)\n",
        "    X_list.append(values)\n",
        "\n",
        "X = np.stack(X_list)  # shape (N, T, 3)\n",
        "print(\"Data shape:\", X.shape)  # e.g. (1394, 100, 3)\n",
        "\n",
        "timesteps = X.shape[1]\n",
        "n_features = X.shape[2]\n",
        "latent_dim = 64\n",
        "\n",
        "# =====================================\n",
        "# 2. Define LSTM Autoencoder (Keras)\n",
        "# =====================================\n",
        "inputs = layers.Input(shape=(timesteps, n_features))\n",
        "\n",
        "# Encoder\n",
        "encoded = layers.LSTM(128, return_sequences=True)(inputs)\n",
        "encoded = layers.LSTM(latent_dim)(encoded)\n",
        "\n",
        "# Decoder\n",
        "decoded = layers.RepeatVector(timesteps)(encoded)\n",
        "decoded = layers.LSTM(128, return_sequences=True)(decoded)\n",
        "outputs = layers.TimeDistributed(layers.Dense(n_features))(decoded)\n",
        "\n",
        "# Build Model\n",
        "autoencoder = models.Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()\n",
        "\n",
        "# =====================================\n",
        "# 3. Train Autoencoder\n",
        "# =====================================\n",
        "history = autoencoder.fit(\n",
        "    X, X,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# 4. Extract embeddings\n",
        "# =====================================\n",
        "# Define encoder model separately\n",
        "encoder = models.Model(inputs, encoded)\n",
        "\n",
        "embeddings = encoder.predict(X)  # shape (N, latent_dim)\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "# =====================================\n",
        "# 5. Clustering\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hLMg5w3BCv49",
        "outputId": "7c2d5e15-1e7b-4a9c-d653-8ab0566a873b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (1395, 501, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m501\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m501\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m67,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ repeat_vector_1 (\u001b[38;5;33mRepeatVector\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m501\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m501\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m501\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │           \u001b[38;5;34m387\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ repeat_vector_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">501</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,195\u001b[0m (844.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,195</span> (844.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m216,195\u001b[0m (844.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,195</span> (844.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 4s/step - loss: 191.6054 - val_loss: 108.7447\n",
            "Epoch 2/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 4s/step - loss: 177.3209 - val_loss: 108.0229\n",
            "Epoch 3/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 4s/step - loss: 182.5917 - val_loss: 107.9078\n",
            "Epoch 4/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 4s/step - loss: 169.0421 - val_loss: 107.6330\n",
            "Epoch 5/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 4s/step - loss: 178.8479 - val_loss: 107.2402\n",
            "Epoch 6/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 4s/step - loss: 180.5170 - val_loss: 104.5921\n",
            "Epoch 7/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 4s/step - loss: 173.8890 - val_loss: 104.8372\n",
            "Epoch 8/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 4s/step - loss: 172.0069 - val_loss: 104.4592\n",
            "Epoch 9/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 4s/step - loss: 173.8118 - val_loss: 104.0937\n",
            "Epoch 10/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 4s/step - loss: 173.1725 - val_loss: 104.2388\n",
            "Epoch 11/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 4s/step - loss: 173.8833 - val_loss: 104.5643\n",
            "Epoch 12/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 4s/step - loss: 173.7768 - val_loss: 105.0334\n",
            "Epoch 13/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 4s/step - loss: 173.5891 - val_loss: 104.3891\n",
            "Epoch 14/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 4s/step - loss: 171.5856 - val_loss: 104.8325\n",
            "Epoch 15/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 4s/step - loss: 173.4920 - val_loss: 103.3279\n",
            "Epoch 16/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 4s/step - loss: 167.9255 - val_loss: 102.8925\n",
            "Epoch 17/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 4s/step - loss: 170.7868 - val_loss: 100.8499\n",
            "Epoch 18/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 4s/step - loss: 164.5695 - val_loss: 102.8943\n",
            "Epoch 19/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 4s/step - loss: 177.6187 - val_loss: 100.9979\n",
            "Epoch 20/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 4s/step - loss: 176.5147 - val_loss: 100.5933\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 329ms/step\n",
            "Embeddings shape: (1395, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvYDb9w_a7hz"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(threshold=np.inf)  # disable truncation\n",
        "print(labels)\n",
        "print(\"Embeddings shape:\", embeddings.shape)   # should be (1329, latent_dim)\n",
        "print(\"First few embeddings:\\n\", embeddings[:5])\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PSZ1g3lbD_Z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Reduce embeddings (high-dim) → 2D with t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=0, perplexity=30, n_iter=1000)\n",
        "embeddings_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(\n",
        "    embeddings_2d[:, 0],\n",
        "    embeddings_2d[:, 1],\n",
        "    c=labels,\n",
        "    cmap=\"tab10\",\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title(\"t-SNE Visualization of Clusters\")\n",
        "plt.xlabel(\"t-SNE Dim 1\")\n",
        "plt.ylabel(\"t-SNE Dim 2\")\n",
        "plt.colorbar(scatter, label=\"Cluster ID\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIQxA32Aj8pzWGRPV3Dv8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}